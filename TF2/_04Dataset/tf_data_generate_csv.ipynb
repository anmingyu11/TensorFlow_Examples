{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)\n",
      "matplotlib 3.1.1\n",
      "numpy 1.17.2\n",
      "pandas 0.25.1\n",
      "sklearn 0.21.3\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state = 7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train_all, y_train_all, random_state = 11)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"generated_csv\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "def save_to_csv(output_dir, data, name_prefix,\n",
    "                header=None, n_parts=10):\n",
    "    path_format = os.path.join(output_dir, \"{}_{:02d}.csv\")\n",
    "    filenames = []\n",
    "    \n",
    "    for file_idx, row_indices in enumerate(\n",
    "        np.array_split(np.arange(len(data)), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filenames.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header + \"\\n\")\n",
    "            for row_index in row_indices:\n",
    "                f.write(\",\".join(\n",
    "                    [repr(col) for col in data[row_index]]))\n",
    "                f.write('\\n')\n",
    "    return filenames\n",
    "\n",
    "train_data = np.c_[x_train_scaled, y_train]\n",
    "valid_data = np.c_[x_valid_scaled, y_valid]\n",
    "test_data = np.c_[x_test_scaled, y_test]\n",
    "header_cols = housing.feature_names + [\"MidianHouseValue\"]\n",
    "header_str = \",\".join(header_cols)\n",
    "\n",
    "train_filenames = save_to_csv(output_dir, train_data, \"train\",\n",
    "                              header_str, n_parts=20)\n",
    "valid_filenames = save_to_csv(output_dir, valid_data, \"valid\",\n",
    "                              header_str, n_parts=10)\n",
    "test_filenames = save_to_csv(output_dir, test_data, \"test\",\n",
    "                             header_str, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['generated_csv/train_00.csv',\n",
       " 'generated_csv/train_01.csv',\n",
       " 'generated_csv/train_02.csv',\n",
       " 'generated_csv/train_03.csv',\n",
       " 'generated_csv/train_04.csv',\n",
       " 'generated_csv/train_05.csv',\n",
       " 'generated_csv/train_06.csv',\n",
       " 'generated_csv/train_07.csv',\n",
       " 'generated_csv/train_08.csv',\n",
       " 'generated_csv/train_09.csv',\n",
       " 'generated_csv/train_10.csv',\n",
       " 'generated_csv/train_11.csv',\n",
       " 'generated_csv/train_12.csv',\n",
       " 'generated_csv/train_13.csv',\n",
       " 'generated_csv/train_14.csv',\n",
       " 'generated_csv/train_15.csv',\n",
       " 'generated_csv/train_16.csv',\n",
       " 'generated_csv/train_17.csv',\n",
       " 'generated_csv/train_18.csv',\n",
       " 'generated_csv/train_19.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: (), types: tf.string>\n"
     ]
    }
   ],
   "source": [
    "filename_dataset = tf.data.Dataset.list_files(train_filenames)\n",
    "print(filename_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'generated_csv/train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generated_csv/train_15.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for file_name in filename_dataset:\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'2.51504373119231,1.0731637904355105,0.5574401201546321,-0.17273513019187772,-0.612912610473286,-0.01909156503651574,-0.5710993036045546,-0.027490309606616956,5.00001'\n",
      "b'0.04971034572063198,-0.8492418886278699,-0.06214699417830008,0.17878747064657746,-0.8025354230744277,0.0005066066922077538,0.6466457006743215,-1.1060793768010604,2.286'\n",
      "b'0.401276648075221,-0.9293421252555106,-0.05333050451405854,-0.1865945262276826,0.6545661895448709,0.026434465728210874,0.9312527706398824,-1.4406417263474771,2.512'\n",
      "b'-0.6672227549433569,-0.04823952235146133,0.34529405473316743,0.5382668657200925,1.8521839533415545,-0.0611253832474835,-0.8417093045554153,1.520484740533546,1.59'\n",
      "b'0.15782311132800697,0.43236189741438374,0.3379948076652917,-0.015880306122244434,-0.3733890577139493,-0.05305245634489608,0.8006134598360177,-1.2359095422966828,3.169'\n",
      "b'1.8444675088321243,0.5124621340420246,0.505783649224786,-0.20645711406004988,-0.021362018052499883,-0.05811312281214649,0.8332732875369839,-1.2658703497187516,4.513'\n",
      "b'-1.453851024367546,1.874166156711919,-1.1315714708271856,0.3611276016530489,-0.3978857847006997,-0.03273859332533962,-0.7390641317809511,0.646627857389904,1.875'\n",
      "b'-0.8757754235423053,1.874166156711919,-0.9487499555702599,-0.09657184824705009,-0.7163432355284542,-0.07790191228558485,0.9825753570271144,-1.4206678547327694,2.75'\n",
      "b'1.5180511450515526,-0.5288409421173064,0.8102470510967439,-0.1921416982863481,0.44135393614167334,0.027335058055345158,-0.8183808561975836,0.8563535093443789,2.898'\n",
      "b'2.2878417437355094,-1.8905449647872008,0.6607106467795992,-0.14964778023694128,-0.06672632728722275,0.44788055801575993,-0.5337737862320228,0.5667323709310584,3.59'\n",
      "b'-0.8109823614533115,0.43236189741438374,-0.09614708870040699,-0.011052490243107498,-0.5884158834865357,-0.15275615510545787,-1.3036125820405071,1.15096811566138,4.889'\n",
      "b'-1.1179501498535522,0.3522616607867429,-0.17415480367337632,0.1029357335256435,-0.24364713330264193,-0.06195252491676357,1.9063819119972951,-1.1210597805120879,0.603'\n",
      "b'0.7751155655229017,1.874166156711919,0.15645971958808144,-0.18905190538070707,-0.6292437617977863,-0.08791603438866835,-0.7483955111240856,0.5717258388347319,4.851'\n",
      "b'-0.3295635160799086,0.9930635538078697,-0.8771740525217612,-0.3636710820906513,-1.1164564429787098,-0.08510593365640572,1.0665577711153127,-1.38571357940702,1.563'\n",
      "b'0.3798565732727743,-1.5701440182766375,0.4541195259524651,-0.13374802152613807,-0.28356772542919806,-0.04747003172530946,-0.3191520613399599,-0.41698080609349797,1.901'\n"
     ]
    }
   ],
   "source": [
    "n_readers = 5\n",
    "dataset = filename_dataset.interleave(\n",
    "    lambda filename :tf.data.TextLineDataset(filename).skip(1) # 第一行是列名\n",
    "    , cycle_length = n_readers\n",
    ")\n",
    "for line in dataset.take(15):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "tf.Tensor(b'4', shape=(), dtype=string)\n",
      "tf.Tensor(5.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.io.decode_csv(str, record_defaults)\n",
    "sample_str = '1,2,3,4,5'\n",
    "record_defaults=[\n",
    "    tf.constant(0, dtype=tf.int32),\n",
    "    0,\n",
    "    np.nan,\n",
    "    \"hello\",\n",
    "    tf.constant([])\n",
    "]\n",
    "parsed_fileds = tf.io.decode_csv(sample_str,record_defaults)\n",
    "for it in parsed_fileds:\n",
    "    print(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "record_defaults:\n",
    "```\n",
    "A list of `Tensor` objects with specific types.\n",
    " Acceptable types are `float32`, `float64`, `int32`, `int64`, `string`.\n",
    " One tensor per column of the input record, with either a scalar default value for that column or an empty vector if the column is required.\n",
    "```\n",
    "\n",
    "```\n",
    "具有特定类型的“张量”对象的列表。\n",
    "可接受的类型有“float32”、“float64”、“int32”、“int64”、“string”。\n",
    "输入记录的每列一个张量，该列可以是标量默认值，也可以是空向量(如果需要该列)。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field 4 is required but missing in record 0! [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "try:\n",
    "    parsed_fileds = tf.io.decode_csv(',,,,' , record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect 5 fields but have 7 in record 0 [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    parsed_fields = tf.io.decode_csv('1,2,3,4,5,6,7', record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['a','b']* 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=113, shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: id=113, shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: id=113, shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: id=113, shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: id=113, shape=(), dtype=float32, numpy=nan>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tf.constant(np.nan)] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_line(line , n_fields = 9):\n",
    "    defs = [tf.constant(np.nan)] * n_fields\n",
    "    parsed_fields = tf.io.decode_csv(line , record_defaults = defs)\n",
    "    x = tf.stack(parsed_fields[0:-1])\n",
    "    y = tf.stack(parsed_fileds[-1])\n",
    "    print(x)\n",
    "    print(y)\n",
    "    return x , y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.9868721   0.8328631  -0.18684709 -0.1488895  -0.45323023 -0.11504996\n",
      "  1.6730974  -0.74654967], shape=(8,), dtype=float32)\n",
      "tf.Tensor(5.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=125, shape=(8,), dtype=float32, numpy=\n",
       " array([-0.9868721 ,  0.8328631 , -0.18684709, -0.1488895 , -0.45323023,\n",
       "        -0.11504996,  1.6730974 , -0.74654967], dtype=float32)>,\n",
       " <tf.Tensor: id=104, shape=(), dtype=float32, numpy=5.0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_csv_line(\n",
    "    b'-0.9868720801669367,0.832863080552588,-0.18684708416901633,-0.14888949288707784,-0.4532302419670616,-0.11504995754593579,1.6730974284189664,-0.7465496877362412,1.138'\n",
    "    , n_fields=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"stack:0\", shape=(8,), dtype=float32)\n",
      "Tensor(\"stack_1:0\", shape=(), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n"
     ]
    }
   ],
   "source": [
    "# 1. filename -> dataset\n",
    "# 2. read file -> dataset -> datasets -> merge\n",
    "# 3. parse csv\n",
    "def csv_reader_dataset(\n",
    "    filenames\n",
    "    , n_readers = 5\n",
    "    , batch_size = 32\n",
    "    , n_parse_threads = 5\n",
    "    , shuffle_buffer_size = int(1e5)\n",
    "):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.interleave( # map reduce\n",
    "        lambda filename : tf.data.TextLineDataset(filename).skip(1)\n",
    "        , cycle_length = n_readers\n",
    "    ) # map 的并行处理 读取数据\n",
    "    # Randomly shuffles the elements of this dataset.\n",
    "    dataset.shuffle(shuffle_buffer_size) # \n",
    "    dataset = dataset.map( # map\n",
    "        parse_csv_line\n",
    "        , num_parallel_calls = n_parse_threads\n",
    "    )\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "train_set = csv_reader_dataset(train_filenames,batch_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=209, shape=(3, 8), dtype=float32, numpy=\n",
      "array([[ 2.5150437 ,  1.0731637 ,  0.5574401 , -0.17273512, -0.6129126 ,\n",
      "        -0.01909157, -0.5710993 , -0.02749031],\n",
      "       [-1.1157656 ,  0.99306357, -0.334192  , -0.06535219, -0.32893205,\n",
      "         0.04343066, -0.12785879,  0.30707204],\n",
      "       [-0.82195884,  1.8741661 ,  0.1821235 , -0.03170019, -0.6011179 ,\n",
      "        -0.14337493,  1.0852206 , -0.8613995 ]], dtype=float32)>, <tf.Tensor: id=210, shape=(3,), dtype=float32, numpy=array([5., 5., 5.], dtype=float32)>)\n",
      "(<tf.Tensor: id=211, shape=(3, 8), dtype=float32, numpy=\n",
      "array([[-1.0775077 , -0.4487407 , -0.5680568 , -0.14269263, -0.09666677,\n",
      "         0.12326469, -0.31448638, -0.4818959 ],\n",
      "       [ 0.15782312,  0.4323619 ,  0.3379948 , -0.01588031, -0.37338907,\n",
      "        -0.05305246,  0.80061346, -1.2359096 ],\n",
      "       [ 1.8444675 ,  0.51246214,  0.5057837 , -0.20645711, -0.02136202,\n",
      "        -0.05811312,  0.8332733 , -1.2658703 ]], dtype=float32)>, <tf.Tensor: id=212, shape=(3,), dtype=float32, numpy=array([5., 5., 5.], dtype=float32)>)\n",
      "(<tf.Tensor: id=213, shape=(3, 8), dtype=float32, numpy=\n",
      "array([[-0.9760555 ,  1.2333642 , -0.39099863, -0.15728481, -0.82612485,\n",
      "        -0.14088781,  1.3604962 , -0.95128185],\n",
      "       [-0.46794146, -0.92934215,  0.11909926, -0.06047011,  0.30344644,\n",
      "        -0.02185189,  1.8737221 , -1.0411643 ],\n",
      "       [-1.2310716 ,  0.91296333, -0.19194563,  0.12851463, -0.1873954 ,\n",
      "         0.1460428 , -0.785721  ,  0.6566148 ]], dtype=float32)>, <tf.Tensor: id=214, shape=(3,), dtype=float32, numpy=array([5., 5., 5.], dtype=float32)>)\n",
      "(<tf.Tensor: id=215, shape=(3, 8), dtype=float32, numpy=\n",
      "array([[ 2.2878418 , -1.890545  ,  0.66071063, -0.14964779, -0.06672633,\n",
      "         0.44788057, -0.5337738 ,  0.56673235],\n",
      "       [-0.81098235,  0.4323619 , -0.09614709, -0.01105249, -0.58841586,\n",
      "        -0.15275615, -1.3036126 ,  1.1509681 ],\n",
      "       [ 0.6289049 , -0.4487407 ,  0.01139045, -0.21388453,  0.13196935,\n",
      "        -0.08002252, -0.8837005 ,  0.88132083]], dtype=float32)>, <tf.Tensor: id=216, shape=(3,), dtype=float32, numpy=array([5., 5., 5.], dtype=float32)>)\n",
      "(<tf.Tensor: id=217, shape=(3, 8), dtype=float32, numpy=\n",
      "array([[-0.06021407,  0.75276285,  0.08359403, -0.06250122, -0.03497131,\n",
      "        -0.02644238,  1.0712235 , -1.3707331 ],\n",
      "       [-0.47966388,  1.8741661 ,  0.05604706, -0.00684981,  0.02944601,\n",
      "        -0.12115399,  1.033898  , -1.3407724 ],\n",
      "       [ 0.3798566 , -1.570144  ,  0.45411953, -0.13374802, -0.28356773,\n",
      "        -0.04747003, -0.31915206, -0.4169808 ]], dtype=float32)>, <tf.Tensor: id=218, shape=(3,), dtype=float32, numpy=array([5., 5., 5.], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for item in train_set.take(5):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :  tf.Tensor(\n",
      "[[ 0.09734604  0.75276285 -0.20218964 -0.19547    -0.40605137  0.00678553\n",
      "  -0.81371516  0.6566148 ]\n",
      " [ 0.4240821   0.91296333 -0.04437482 -0.15297213 -0.24727628 -0.10539167\n",
      "   0.86126745 -1.335779  ]\n",
      " [ 0.63034356  1.8741661  -0.06713215 -0.12543367 -0.19737554 -0.02272263\n",
      "  -0.69240725  0.72652334]], shape=(3, 8), dtype=float32)\n",
      "y :  tf.Tensor([5. 5. 5.], shape=(3,), dtype=float32)\n",
      "x :  tf.Tensor(\n",
      "[[ 0.48530516 -0.8492419  -0.06530126 -0.02337966  1.4974351  -0.07790658\n",
      "  -0.90236324  0.78145146]\n",
      " [ 2.5150437   1.0731637   0.5574401  -0.17273512 -0.6129126  -0.01909157\n",
      "  -0.5710993  -0.02749031]\n",
      " [-1.4803331  -0.68904144 -0.35624704 -0.17255889 -0.82158846 -0.13823092\n",
      "   1.9157133  -1.0211904 ]], shape=(3, 8), dtype=float32)\n",
      "y :  tf.Tensor([5. 5. 5.], shape=(3,), dtype=float32)\n",
      "x :  tf.Tensor(\n",
      "[[ 0.4369235  -1.9706452  -0.16642106  0.05486205 -0.8379196  -0.1323988\n",
      "  -0.99567705  0.94124246]\n",
      " [ 1.6312258   0.35226166  0.04080576 -0.14088951 -0.4632104  -0.06751624\n",
      "  -0.82771224  0.59669316]\n",
      " [-0.7432054   0.91296333 -0.64432025 -0.1479097   0.7398511   0.11427691\n",
      "  -0.7950524   0.68158215]], shape=(3, 8), dtype=float32)\n",
      "y :  tf.Tensor([5. 5. 5.], shape=(3,), dtype=float32)\n",
      "x :  tf.Tensor(\n",
      "[[ 1.8444675   0.51246214  0.5057837  -0.20645711 -0.02136202 -0.05811312\n",
      "   0.8332733  -1.2658703 ]\n",
      " [ 0.21174629  1.153264   -0.25077614 -0.25649872 -0.6473895   0.01759022\n",
      "   0.7959478  -1.1510206 ]\n",
      " [-0.49303812 -1.570144   -0.6933898   0.16277646  0.32794318 -0.08806529\n",
      "  -0.86503774  0.6366409 ]], shape=(3, 8), dtype=float32)\n",
      "y :  tf.Tensor([5. 5. 5.], shape=(3,), dtype=float32)\n",
      "x :  tf.Tensor(\n",
      "[[ 0.04049226 -0.68904144 -0.4437985   0.02237459 -0.22187227 -0.14828503\n",
      "  -0.8883662   0.6366409 ]\n",
      " [-0.8246763  -0.04823952 -0.34486583 -0.08477587  0.5012348  -0.0347\n",
      "   0.5300035  -0.08741193]\n",
      " [-0.81098235  0.4323619  -0.09614709 -0.01105249 -0.58841586 -0.15275615\n",
      "  -1.3036126   1.1509681 ]], shape=(3, 8), dtype=float32)\n",
      "y :  tf.Tensor([5. 5. 5.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for x_batch,y_batch in train_set.take(5):\n",
    "    print('x : ', x_batch)\n",
    "    print('y : ', y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"stack:0\", shape=(8,), dtype=float32)\n",
      "Tensor(\"stack_1:0\", shape=(), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
      "Tensor(\"stack:0\", shape=(8,), dtype=float32)\n",
      "Tensor(\"stack_1:0\", shape=(), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
      "Tensor(\"stack:0\", shape=(8,), dtype=float32)\n",
      "Tensor(\"stack_1:0\", shape=(), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_set = csv_reader_dataset(train_filenames,\n",
    "                               batch_size = batch_size)\n",
    "valid_set = csv_reader_dataset(valid_filenames,\n",
    "                               batch_size = batch_size)\n",
    "test_set = csv_reader_dataset(test_filenames,\n",
    "                              batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(30,activation='relu',input_shape=[8])\n",
    "        ,keras.layers.Dense(1)\n",
    "    ]\n",
    ")\n",
    "model.compile(loss='mean_squared_error',optimizer='sgd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 348 steps, validate for 120 steps\n",
      "Epoch 1/100\n",
      "348/348 [==============================] - 3s 9ms/step - loss: 2.5366 - val_loss: 0.2177\n",
      "Epoch 2/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.1930 - val_loss: 0.1203\n",
      "Epoch 3/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.2502 - val_loss: 0.0537\n",
      "Epoch 4/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.0389 - val_loss: 0.0309\n",
      "Epoch 5/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.0224 - val_loss: 0.0225\n",
      "Epoch 6/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0205 - val_loss: 0.0270\n",
      "Epoch 7/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.0156 - val_loss: 0.0148\n",
      "Epoch 8/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0096 - val_loss: 0.0110\n",
      "Epoch 9/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 10/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0078 - val_loss: 0.0076\n",
      "Epoch 11/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0062 - val_loss: 0.0070\n",
      "Epoch 12/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 13/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0055 - val_loss: 0.0056\n",
      "Epoch 14/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0059 - val_loss: 0.0071\n",
      "Epoch 15/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0047 - val_loss: 0.0057\n",
      "Epoch 16/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0043 - val_loss: 0.0046\n",
      "Epoch 17/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0070\n",
      "Epoch 18/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 19/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 20/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0028 - val_loss: 0.0033\n",
      "Epoch 21/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0034 - val_loss: 0.0036\n",
      "Epoch 22/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0028 - val_loss: 0.0031\n",
      "Epoch 23/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0024 - val_loss: 0.0028\n",
      "Epoch 24/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0027 - val_loss: 0.0036\n",
      "Epoch 25/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.0024 - val_loss: 0.0031\n",
      "Epoch 26/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 27/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0025 - val_loss: 0.0038\n",
      "Epoch 28/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.0022 - val_loss: 0.0030\n",
      "Epoch 29/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 30/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 31/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0018 - val_loss: 0.0028\n",
      "Epoch 32/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 33/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 34/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 35/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 36/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 37/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 38/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 39/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 40/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 41/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 42/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 43/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 44/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 8.4709e-04 - val_loss: 0.0013\n",
      "Epoch 45/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.0011 - val_loss: 0.0023\n",
      "Epoch 46/100\n",
      "348/348 [==============================] - 3s 7ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 47/100\n",
      "348/348 [==============================] - 2s 7ms/step - loss: 9.4053e-04 - val_loss: 0.0015\n",
      "Epoch 48/100\n",
      "348/348 [==============================] - 3s 9ms/step - loss: 8.9089e-04 - val_loss: 0.0013\n",
      "Epoch 49/100\n",
      "348/348 [==============================] - 3s 8ms/step - loss: 8.6366e-04 - val_loss: 0.0011\n",
      "Epoch 50/100\n",
      "348/348 [==============================] - 3s 8ms/step - loss: 8.4388e-04 - val_loss: 0.0015\n",
      "Epoch 51/100\n",
      "348/348 [==============================] - 3s 7ms/step - loss: 7.7894e-04 - val_loss: 0.0012\n",
      "Epoch 52/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 7.5406e-04 - val_loss: 0.0013\n",
      "Epoch 53/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 6.8819e-04 - val_loss: 9.7464e-04\n",
      "Epoch 54/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 7.8529e-04 - val_loss: 0.0012\n",
      "Epoch 55/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 6.8547e-04 - val_loss: 0.0011\n",
      "Epoch 56/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 6.2118e-04 - val_loss: 0.0011\n",
      "Epoch 57/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 7.0808e-04 - val_loss: 0.0017\n",
      "Epoch 58/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 7.1555e-04 - val_loss: 0.0011\n",
      "Epoch 59/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 8.3965e-04 - val_loss: 0.0012\n",
      "Epoch 60/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 6.6378e-04 - val_loss: 0.0010\n",
      "Epoch 61/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 5.8644e-04 - val_loss: 8.7553e-04\n",
      "Epoch 62/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 6.0857e-04 - val_loss: 0.0020\n",
      "Epoch 63/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 7.4910e-04 - val_loss: 0.0014\n",
      "Epoch 64/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 6.2436e-04 - val_loss: 0.0013\n",
      "Epoch 65/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 6.4109e-04 - val_loss: 0.0011\n",
      "Epoch 66/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 5.1145e-04 - val_loss: 9.3945e-04\n",
      "Epoch 67/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 4.8369e-04 - val_loss: 8.2112e-04\n",
      "Epoch 68/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 6.4329e-04 - val_loss: 0.0014\n",
      "Epoch 69/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 4.9920e-04 - val_loss: 0.0011\n",
      "Epoch 70/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 5.1575e-04 - val_loss: 8.8594e-04\n",
      "Epoch 71/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 5.0945e-04 - val_loss: 8.0663e-04\n",
      "Epoch 72/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 4.1021e-04 - val_loss: 7.3476e-04\n",
      "Epoch 73/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 5.2723e-04 - val_loss: 0.0011\n",
      "Epoch 74/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 4.9196e-04 - val_loss: 9.8132e-04\n",
      "Epoch 75/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 4.4941e-04 - val_loss: 8.7354e-04\n",
      "Epoch 76/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 3.9983e-04 - val_loss: 7.1212e-04\n",
      "Epoch 77/100\n",
      "348/348 [==============================] - 2s 7ms/step - loss: 3.7626e-04 - val_loss: 7.5384e-04\n",
      "Epoch 78/100\n",
      "348/348 [==============================] - 3s 7ms/step - loss: 3.7102e-04 - val_loss: 7.0670e-04\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - 2s 6ms/step - loss: 4.6685e-04 - val_loss: 9.9629e-04\n",
      "Epoch 80/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 4.3863e-04 - val_loss: 7.9698e-04\n",
      "Epoch 81/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 3.7999e-04 - val_loss: 6.8749e-04\n",
      "Epoch 82/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 3.9031e-04 - val_loss: 6.8537e-04\n",
      "Epoch 83/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 2.9860e-04 - val_loss: 7.1964e-04\n",
      "Epoch 84/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 3.5536e-04 - val_loss: 7.1162e-04\n",
      "Epoch 85/100\n",
      "120/348 [=========>....................] - ETA: 0s - loss: 2.5215e-04"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_set\n",
    "    ,validation_data=valid_set\n",
    "    ,steps_per_epoch=11160 // batch_size\n",
    "    ,validation_steps= 3870 // batch_size\n",
    "    ,epochs = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_set,steps = 5160 // batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
